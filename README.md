# EcommPipelineandAnalysis


ğŸ“Œ Project Overview

This project implements a real-time data pipeline for processing e-commerce orders using Databricks, Delta Lake, and Structured Streaming.
It follows the Medallion Architecture (Bronze â†’ Silver â†’ Gold) with data quality checks and Unity Catalog governance to enable real-time analytics and dashboards.

ğŸš€ Architecture

Data Source: Simulated real-time orders using Faker scripts

Ingestion: Databricks Auto Loader for incremental streaming ingestion

Processing: Structured Streaming with Bronze, Silver, and Gold Delta tables

Governance: Unity Catalog for fine-grained security and auditing

Analytics: Databricks SQL for interactive dashboards and reporting

ğŸ—ï¸ Project Structure
/RealTimeEcom/
    â”œâ”€â”€ Notebooks/
    â”‚   â”œâ”€â”€ 01_setup_unity_catalog       # Unity Catalog and schema setup
    â”‚   â”œâ”€â”€ 02_data_ingestion_bronze      # Auto Loader for raw data
    â”‚   â”œâ”€â”€ 03_silver_transformations     # Cleansing and enrichment
    â”‚   â”œâ”€â”€ 04_gold_aggregations          # Final metrics & analytics tables
    â”‚   â”œâ”€â”€ 05_dashboards                 # Databricks SQL dashboards
    â”‚
    â”œâ”€â”€ StreamingJobs/
    â”‚   â”œâ”€â”€ orders_stream.py              # Orders ingestion stream
    â”‚   â”œâ”€â”€ order_items_stream.py         # Order items ingestion stream
    â”‚   â””â”€â”€ utils.py                      # Common streaming functions
    â”‚
    â”œâ”€â”€ BatchJobs/
    â”‚   â””â”€â”€ dim_tables_load.py            # Dimension tables load
    â”‚
    â”œâ”€â”€ Config/
    â”‚   â”œâ”€â”€ dlt_config.yml                # Delta Live Tables configs
    â”‚   â””â”€â”€ spark_conf.json               # Spark streaming configs
    â”‚
    â”œâ”€â”€ FakerScripts/
    â”‚   â””â”€â”€ generate_orders.py            # Simulates real-time orders
    â”‚
    â””â”€â”€ Tests/
        â”œâ”€â”€ test_data_quality.py          # Data quality checks
        â””â”€â”€ test_streaming_logic.py       # Streaming logic validation

ğŸ§° Tech Stack

Databricks (Structured Streaming, Delta Lake, Unity Catalog)

Python (Faker for data generation, PySpark for processing)

Delta Live Tables (DLT) for declarative pipelines

Databricks SQL for dashboards & analytics

GitHub for version control & CI/CD

ğŸ“Š Key Features

Real-time ingestion using Auto Loader

Data quality enforcement with DLT expectations

Medallion Architecture for data refinement (Bronze â†’ Silver â†’ Gold)

End-to-end governance via Unity Catalog

Interactive analytics with Databricks SQL dashboards

ğŸ† Outcomes

Real-time order metrics: sales trends, order counts, revenue analytics

Improved data reliability with quality checks

Easy CI/CD deployment with GitHub integration
